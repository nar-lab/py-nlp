{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python kutuphaneleri\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import snowballstemmer\n",
    "import re\n",
    "from apyori import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#benim dosyalarimin bulundugu dizin\n",
    "main_dir = \"C:\\\\Users\\\\Ali.CABUKEL\\\\Desktop\\\\py-nlp\\\\\"\n",
    "#okunacak ve yazilacak dosya isimleri\n",
    "#tum twit ve entrylerin bulundugu excel dosyasi\n",
    "corpus_file = main_dir + \"derlem.xlsx\"\n",
    "#stop word listesi\n",
    "stop_file = main_dir + \"tr-stop.txt\"\n",
    "#finalde yazilacak dosya isimleri\n",
    "stg_file = main_dir + \"stg.xlsx\"\n",
    "bow_file = main_dir + \"bow.csv\"\n",
    "tf_file = main_dir + \"tf.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words dosyasinin okunmasi ve python set nesnesine atanmasi\n",
    "#set nesnesi icerisinde her degerden bir tane bulunur\n",
    "word_set = set()\n",
    "with open(stop_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    words = f.readlines()\n",
    "for word in words:\n",
    "    word_set.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excel dosyasindan twitler ve entry'ler okunup birlesitiriliyor (union)\n",
    "twits = pd.read_excel(corpus_file,names = [\"content\"],sheet_name=\"Twitter\", header=None)\n",
    "eksi = pd.read_excel(corpus_file, names = [\"content\", \"header\"], sheet_name = \"Eksisozluk\", header=None)\n",
    "corpus = pd.Series(np.r_[twits.content, eksi.content]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#turkish stemmer\n",
    "stemmer = snowballstemmer.stemmer('turkish');\n",
    "#dokumanlarda gecen kelimelerin koklerinin referans listesi\n",
    "bag_of_words = dict()\n",
    "#her dokumana bir id degeri atanacak\n",
    "doc_ix = 0\n",
    "#her kelimeye bir id degeri atanacak\n",
    "term_ix = 0\n",
    "#finalde dokuman id'ler kelime kokleri ve frekans degerlerini tutacak lsite\n",
    "stg_list = []\n",
    "#tum twit ve entry'ler okunuyor\n",
    "#[:1000] ile ilk 1000 tane dokuman okunuyor. \n",
    "append_file = False\n",
    "for sentence in corpus:\n",
    "    #final dosyaya eklensin mi? gecerli kelime(ler) var mi?\n",
    "    #bu durumda dokuman id'yi artir\n",
    "    if append_file:\n",
    "        doc_ix += 1\n",
    "    #baslangic deger olumsuz\n",
    "    append_file = False\n",
    "    #cumle bosluga gore ayristiriliyor, tokenize\n",
    "    tokens = sentence.lower().split(\" \")\n",
    "    #stop listesi ile farki aliniyor\n",
    "    tokens = list(set(tokens).difference(word_set))\n",
    "    #icerisinde ozel yeni satir karakteri varsa siliniyor\n",
    "    tokens = [t.replace(\"\\r\",\"\").replace(\"\\n\",\"\") for t in tokens]\n",
    "    #ayristirilan kelimelerin kokleri bulunuyor\n",
    "    stems = stemmer.stemWords(tokens)\n",
    "    #cumledeki kelime uzunlugu kadar dongu\n",
    "    for i in range(len(tokens)):\n",
    "        #numerik ve ozel karakter sayisi cumle uzunluguna esit mi kontrolu\n",
    "        #esiste gecerli karakter yok ve diger elemente gec\n",
    "        num_and_spec = len(re.sub('[\\w]+' ,'', stems[i])) + sum(c.isdigit() for c in stems[i])\n",
    "        word_len = len(stems[i])\n",
    "        if word_len > 2 and word_len > num_and_spec:\n",
    "            #https:// http:// prefix ile baslayan dokumanlari atla\n",
    "            if stems[i].startswith(\"https://\") is False and stems[i].startswith(\"http://\") is False:\n",
    "                #mention ve hashtag'leri ele\n",
    "                if stems[i].startswith(\"#\") is False and stems[i].startswith(\"@\") is False:\n",
    "                    #token eger bag of words listesinde varsa guncelle\n",
    "                    #yoksa listeye ekle\n",
    "                    #dictionary'nin key alani kelime koku\n",
    "                    #value alani ise bu koku hangi kelimeden buldu?\n",
    "                    #guncellenen deger value kismi\n",
    "                    if stems[i] in bag_of_words.keys():\n",
    "                        bag_of_words[stems[i]].update([tokens[i]])\n",
    "                        append_file = True\n",
    "                    else:\n",
    "                        bag_of_words[stems[i]] = {tokens[i]}\n",
    "                        append_file = True\n",
    "                        #eger dosyaya eklemeye deger birseyler varsa final listeye ekle\n",
    "                        #bu liste dosyaya yazilacak ve term doc freq matrisi olusturulacak\n",
    "                        if append_file:\n",
    "                            stg_list.append({\"doc_idx\":doc_ix, \"root\": stems[i], \"sent\": sentence})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#final listeyi pandas dataframe'ine cevir\n",
    "corp_df = pd.DataFrame(stg_list)\n",
    "#bag_of_words dictionary'sini kelime koku, koke karsilik gelen liste ve kokun indexi seklinde yeni bir listeye cevir\n",
    "bag_of_word_list = list(zip(list(range(len(bag_of_words))),list(bag_of_words.keys()), list(bag_of_words.values())))\n",
    "#bag_of_words listesinden pandas data frame'ine donusum\n",
    "bow_df = pd.DataFrame(bag_of_word_list, columns = [\"term_idx\", \"root\",\"wordlist\"])\n",
    "#term document detay listesi ile bag of words listesini kelime kokune gore join'le\n",
    "#eslesen listeyi document id ve term id'ye gore gruplayip frekansini hesapla\n",
    "term_doc_freq_df = pd.merge(corp_df, bow_df, on = \"root\", how = \"inner\").groupby([\"doc_idx\",\"term_idx\"]).size()\n",
    "#gruplama sonrasi indexin resetlenmesi ve kolonlarin isimlendirilmesi\n",
    "term_doc_freq_df = term_doc_freq_df.reset_index()\n",
    "term_doc_freq_df.columns = [\"doc_idx\",\"term_idx\",\"freq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bag of words listesini kok kelime, koke karsilik gelen kelimeler ve kelime indexi olarak dosyaya yaz\n",
    "bow_df.to_csv(bow_file, sep=\"\\t\", index=None)\n",
    "#detay tabloyu dosyaya yaz\n",
    "#document id, kelime koku, document icerigi bilgileri\n",
    "corp_df.to_excel(stg_file)\n",
    "#term document freq listesi\n",
    "#term ve document id'ler olarak bulunuyor.\n",
    "#bu liste uzerinden association ya da link analizi yapilabilir.\n",
    "term_doc_freq_df.to_csv(tf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#term document frekans tablosunun frekansa gore azalan siralamasi\n",
    "term_doc_freq_df.sort_values(ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#term document sparse matrix\n",
    "#satirlar document, kolonlar term index, degerler frekans degeri\n",
    "pd.pivot_table(term_doc_freq_df, index = \"doc_idx\", columns=[\"term_idx\"], values=\"freq\", fill_value=0,aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# association rule mining apriori algoritmasının pythonda bir ornegi asagidaki yapilabiliyor.\n",
    "# Burda kritik olan ayar min_lift, support ve conf degerleri\n",
    "# Eger guclu birliktelikler yoksa gecerli birliktelik kurali bulamiyor.\n",
    "tf_vertical = term_doc_freq_df.groupby(\"doc_idx\")[\"term_idx\"].agg(lambda x: list(\"\".join(x.astype(str)))).tolist()\n",
    "rules = apriori(tf_vertical, min_support = 0.003, min_confidence = 0.2, min_lift = 1, min_length = 2)\n",
    "results = list(rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPEnv",
   "language": "python",
   "name": "nlpenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
